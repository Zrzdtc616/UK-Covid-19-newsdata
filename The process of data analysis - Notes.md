# 新闻数据分析的过程

## 一些想法
1、通过时间线和发布数量、频率判断媒体重视程度<br>
（关键词权重计算，然后生成一张图片，图片上是一个一个的词，文本中出现次数越多，这个词在图片上的大小就越大，可能可以用来分析文章的感情倾向？）<br>
2、通过新闻描述判断文本情感（问题：新闻通常是不带情感的？而且用一句话的新闻描述来概括整篇情感态度可能有些偏差）<br>
3、时间线+关键词：提取标题和描述中的关键词，看每个时间段里媒体最关注的是什么方面，高频关键词<br>
4、不同媒体之间文本的相似度，看哪些报刊报道的与众不同<br>
5、新闻的数量趋势图，或者所占某个媒体的版面比例<br>
6、新闻热点演化分析：考虑新闻关键词的共现关系，以此来构建网络，再用社会网络分析法分析关键节点、主题演化等<br>
7、共现分析：看与新冠肺炎同时出现的关键词的数量，应该也会随着时间变得越来越多，因为疫情影响面越来越广，某些方面影响越深出现次数应该越多。可以做一个关键词的共现网络，运用复杂网络的方法进行分析，应该能得到更多结论。
这个我自己又看了一下各个报纸的板块，发现每个板块下（比如Culture、Money里都有关于coronavirus的新闻，可以将这个与每个板块的领域来结合分析）<br>
8、新闻推荐：读者在看到一篇新闻之后，可能还想了解更多，可以根据title的相似度来对author进行聚类分析，然后再根据类别进行个性化推荐。（比如authorA和authorB都报道事件C，那authorA和authorB将属于同一簇，我阅读了A的新闻后，就会自动跟我推荐B的报道。）<br><br>

## 2020.5.2 周六
文本预处理是要文本处理成计算机能识别的格式，是文本分类、文本可视化、文本分析等研究的重要步骤。具体流程包括文本分词、去除停用词、词干抽取(词形还原)、文本向量表征、特征选择等步骤，以消除脏数据对挖掘分析结果的影响。<br><br>

今天学习基于python nltk库来进行英文文本的预处理<br>
-参考网站：https://www.jianshu.com/p/9d232e4a3c28<br>
-先在终端安装nltk库，输入pip install nltk，然后弹出来要安装一个什么软件<br>
-安了一二十分钟之后，我又输入了一次pip install nltk<br>
-然后在python编辑器里import nltk 才没有报错，至此应该可以使用这个库了。<br><br>

今天学习的内容都在study.py那个文件里了，有很多细节需要注意，比如画图要导入两个库：turtle和m开头的那个库。然后还有各种库需要导。学习了NLTK库的一些基本使用方法，还没学完，感觉自己还需要把W3School python tutorial里面关于机器学习和numpy的部分给学了才能顺利的做数据分析，哭死在电脑前，我为啥给自己找了这个论文选题？？？？？<br><br>

## 2020.5.3 周日
帮晴写了获取新闻关键词的代码，感觉csv文件好难处理哦，没有excel好搞。<br><br>

## 2020.5.4 周一
词干提取和词形还原参考网站：https://easyai.tech/ai-definition/stemming-lemmatisation/ <br><br>
![avatar](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-15-weizhi.png)<br>
读取文本-》过滤特殊符号、常见缩写单词-〉分词-》词形还原-〉统计词频<br><br>

-今天刚开始想用一条爬出来的新闻描述的文本来试一下分词和关键词提取，结果发现每一条新闻描述都好短哦，感觉没法提取关键词，还是得爬具体的新闻文本才能更深入进行文本分析。又看了一下卫报的条款，上面说新闻内容可以用于个人和非商业，但是不能建立数据库，我觉得可以钻个空子，就进行文本分析，分析完把数据库给删了，或者一条一条分析，不建立数据库hh<br>
-然后再数据爬取那一部分还有一个问题就是数据的选择，感觉如果选coronavirus outbreak板块下面的新闻实在是太多了，到今天为止一共将近10000条数据。我在考虑换成today’s paper下面的关于新冠的新闻，一天的新闻大概36条，5个月加在一起才5000多条。<br>
-或者这样，进行具体文本分析的时候选择today’s paper下面的，进行一些纯数据上的分析的时候可以用outbreak下面的 <br><br>

-今天尝试了一大段英文的分词、去除停用词、删除一些标点（到这一步的时候还得看一下正则表达式的知识）、词形还原、统计词频 <br><br>

-明天要解决的问题：高频词里出现了the和would，在数据处理的时候要把这些词给去了，还有as <br><br>

## 2020.5.5 周二
-解决了添加停用词“The”和“would”的问题（“as”本来在停用词里就有）。<br>
-词云分析参考网站：https://www.cnblogs.com/wkfvawl/p/11585986.html <br>
-tf-idf计算参考网站：理论：https://blog.csdn.net/zrc199021/article/details/53728499 <br>
-python代码：https://blog.csdn.net/u012052268/article/details/79560768 <br><br>

## 2020.6.4 周四
-尝试了用获取到的数据库中的新闻文章内容来做分词和获取高频词<br>
-因为我在数据爬取阶段的数据一直在变，经常删除表格，又重新生成表格之类的，但是重新生成的表格的编码方式又变成了latin1，遇到特殊字符的话会报错，还是要通过之前的方法修改编码方式为utf-8才行<br>
-今天在尝试编代码过程中还遇到了一个错误，就是在我尝试在数据库里把出版时间后面多余的部分删除，只留下类似“2020-06-04”这样形式的日期的时候，数据库会出现问题，报错，谷歌了一下好像跟游标有关系，又是一个知识点，再一次责怪自己本科为啥不好好学？？<br><br>

## 2020.6.15 周一
-搜相关论文，谷歌学术，搜索词：News data analysis python <br><br>

## 2020.6.22 周一
-今天尝试简单初步的分析卫报和太阳报的数据，整理了一个“英国报纸数据初步简单分析.xls” <br>
-里面包括了最早报道的时间、每个月每个星期里报道的数量 <br>
-首先把数据清理一下，卫报表格存在一些除了标题和url其他内容都没爬到的信息，整体删除这些条信息。（选择发布时间列-查找和选择-查找条件-空值-确定-然后右键删除“整行”）<br>
-然后给每个月的每个星期分了Sheet，重新标了id（双击十字即可自动填充整列）<br>
-然后删除了卫报的keyword8、9、10（因为最高只到7）（在某一个sheet右键-“选择所有工作表”-在某一个工作表内删除这三列-就可以看到所有的sheet都删除了这三列）<br>
-1月份和2月份的卫报整理完了<br><br>

## 2020.6.23 周二
-按星期整理了卫报3、4、5月的数据
